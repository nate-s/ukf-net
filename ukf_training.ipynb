{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/foo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import csv\n",
    "import attn_modules as attn\n",
    "import ukf_modules as ukf_\n",
    "import odometry as odom\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def quat2eul(qw,qx,qy,qz):\n",
    "    \n",
    "    # roll (x-axis rotation)\n",
    "    sinr_cosp = 2 * (qw * qx + qy * qz);\n",
    "    cosr_cosp = 1 - 2 * (qx * qx + qy * qy);\n",
    "    roll = np.arctan2(sinr_cosp, cosr_cosp);\n",
    "\n",
    "    # pitch (y-axis rotation)\n",
    "    sinp = 2 * (qw * qy - qz * qx);\n",
    "    #if (abs(sinp) >= 1):\n",
    "     #   pitch = copysign(M_PI / 2, sinp) # use 90 degrees if out of range\n",
    "    #else:\n",
    "    pitch = np.arcsin(sinp);\n",
    "\n",
    "    # yaw (z-axis rotation)\n",
    "    siny_cosp = 2 * (qw * qz + qx * qy);\n",
    "    cosy_cosp = 1 - 2 * (qy * qy + qz * qz);\n",
    "    yaw = np.arctan2(siny_cosp, cosy_cosp);\n",
    "\n",
    "    return roll, pitch, yaw\n",
    "\n",
    "\n",
    "def c(x):\n",
    "    return torch.cos(x.cuda())\n",
    "def s(x):\n",
    "    return torch.sin(x.cuda())\n",
    "\n",
    "\n",
    "def integration_loss(x_hat_, xdot,xdotdot,pdot,pdotdot, id, dt, loss_metric):\n",
    "    x = x_hat_[0,0,0:2,0:3]\n",
    "    p = x_hat_[0,0,0:2,9:12]\n",
    "    pred = torch.concat((x,xdot,xdotdot,p,pdot,pdotdot), dim=1).reshape(1,2,1,18)\n",
    "    \n",
    "    x_hat_1 = integrator(pred[0,0,0,:].reshape(1,1,1,18), dt)\n",
    "    x_2 = pred[0,1,0,:]\n",
    "    m = (x_2 + x_hat_1)/2\n",
    "    pred[0,1,0,3:9] = m[0,0,0,3:9]\n",
    "    pred[0,1,0,12:18] = m[0,0,0,12:18]\n",
    "    x_hat_2 = integrator(pred[0,1,0,:].reshape(1,1,1,18), dt)\n",
    "    \n",
    "    x_hat_ = torch.concat((x_hat_1, x_hat_2), dim=2)\n",
    "\n",
    "   \n",
    "    gt_Y_x = torch.concat((gt_x_[:,id]-gt_x_[:,id-3], gt_xdot_[:,id]))\n",
    "    gt_Y_o = torch.from_numpy(np.unwrap(gt_o_[id,:]-gt_o_[id-4,:])).float()\n",
    "    x_hat_x = x_hat_2[0,0,0,0:6]\n",
    "    x_hat_o = x_hat_2[0,0,0,9:12]\n",
    "\n",
    "    l = loss_metric(x_hat_x, gt_Y_x.cuda()) + loss_metric(x_hat_o, gt_Y_o.cuda())\n",
    "    \n",
    "    return l, x_hat_2\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data loading block Vicon 01 medium\n",
    "\n",
    "\"\"\" \n",
    "import matplotlib.image as img\n",
    "image_dim = 448 # temp dim i believe\n",
    "        \n",
    "with open('data/cam1/data.csv', mode ='r')as file_im: \n",
    "    csvFile_pose = csv.DictReader(file_im)\n",
    "    names = []\n",
    "    images = []\n",
    "    timestamps_images = []\n",
    "    for col in csvFile_pose:\n",
    "        timestamps_images.append(col['#timestamp [ns]'])\n",
    "        names.append(col['filename'])\n",
    "\n",
    "\n",
    "_images = torch.empty((2000, image_dim, image_dim), dtype=torch.double)\n",
    "times = np.zeros(2000)\n",
    "count = 1337\n",
    "for i1 in range(1710):\n",
    "    path = ('data/cam1/data/' + names[i1])\n",
    "    image = img.imread(path)\n",
    "    tensor_image = torch.from_numpy(image[16:480-16, 152:752-152]).unsqueeze(0)\n",
    "    _images[i1] = tensor_image[0].cpu()\n",
    "    times[i1] = timestamps_images[i1]\n",
    "\n",
    "        \n",
    "\n",
    "with open('data/cam1/data_odom.csv', mode ='r')as file_pose:\n",
    "    # reading the CSV file\n",
    "    csvFile_pose = csv.DictReader(file_pose)\n",
    "\n",
    "    pose_timestamp = []\n",
    "    q_RS_w = []\n",
    "    q_RS_x = []\n",
    "    q_RS_y = []\n",
    "    q_RS_z = []\n",
    "    b_w_RS_S_x = []\n",
    "    b_w_RS_S_y = []\n",
    "    b_w_RS_S_z = []\n",
    "    \n",
    "    p_RS_R_x = []\n",
    "    p_RS_R_y = []\n",
    "    p_RS_R_z = []\n",
    "    q_RS_z = []\n",
    "    v_RS_R_x = []\n",
    "    v_RS_R_y = []\n",
    "    v_RS_R_z = []\n",
    "    b_a_RS_R_x = []\n",
    "    b_a_RS_R_y = []\n",
    "    b_a_RS_R_z = []\n",
    "    \n",
    "    i = 0\n",
    "    n = 36382 #(number of datapoints)\n",
    "    # displaying the contents of the CSV file\n",
    "    for col in csvFile_pose:\n",
    "        pose_timestamp.append(col['#timestamp'])\n",
    "        p_RS_R_x.append(col[' p_RS_R_x [m]'])\n",
    "        p_RS_R_y.append(col[' p_RS_R_y [m]'])\n",
    "        p_RS_R_z.append(col[' p_RS_R_z [m]'])\n",
    "        q_RS_w.append(col[' q_RS_w []'])\n",
    "        q_RS_x.append(col[' q_RS_x []'])\n",
    "        q_RS_y.append(col[' q_RS_y []'])\n",
    "        q_RS_z.append(col[' q_RS_z []'])\n",
    "        \n",
    "        b_w_RS_S_x.append(col[' b_w_RS_S_x [rad s^-1]'])\n",
    "        b_w_RS_S_y.append(col[' b_w_RS_S_y [rad s^-1]'])\n",
    "        b_w_RS_S_z.append(col[' b_w_RS_S_z [rad s^-1]'])\n",
    "        v_RS_R_x.append(col[' v_RS_R_x [m s^-1]'])\n",
    "        v_RS_R_y.append(col[' v_RS_R_y [m s^-1]'])\n",
    "        v_RS_R_z.append(col[' v_RS_R_z [m s^-1]'])\n",
    "        b_a_RS_R_x.append(col[' b_a_RS_S_x [m s^-2]'])\n",
    "        b_a_RS_R_y.append(col[' b_a_RS_S_y [m s^-2]'])\n",
    "        b_a_RS_R_z.append(col[' b_a_RS_S_z [m s^-2]'])\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "pt_01 = np.zeros((2,1337))\n",
    "for i1 in range(1337):\n",
    "    pt_01[0][i1] = 10+i1\n",
    "    pt_01[1][i1] = 1+10*i1\n",
    "\n",
    "if 1:\n",
    "\n",
    "    gt_x = np.zeros((3,1337))\n",
    "    gt_o = np.zeros((4,1337))\n",
    "    gt_xdot = np.zeros((3,1337))\n",
    "    gt_xdotdot = np.zeros((3,1337))\n",
    "    gt_odot = np.zeros((3,1337))\n",
    "    for i1 in range(1337):\n",
    "        gt_x[0][i1] =  p_RS_R_x[int(pt_01[1][i1])] #1403715274372140000\n",
    "        gt_x[1][i1] =  p_RS_R_y[int(pt_01[1][i1])] #1403715274372140000\n",
    "        gt_x[2][i1] =  p_RS_R_z[int(pt_01[1][i1])] #1403715274372140000\n",
    "\n",
    "        gt_xdot[0][i1] =  v_RS_R_x[int(pt_01[1][i1])] #1403715274372140000\n",
    "        gt_xdot[1][i1] =  v_RS_R_y[int(pt_01[1][i1])] #1403715274372140000\n",
    "        gt_xdot[2][i1] =  v_RS_R_z[int(pt_01[1][i1])] #1403715274372140000\n",
    "\n",
    "        #gt_xdotdot[0][i1] =  b_a_RS_R_x[int(pt_01[1][i1])] #1403715274372140000\n",
    "        #gt_xdotdot[1][i1] =  b_a_RS_R_y[int(pt_01[1][i1])] #1403715274372140000\n",
    "        #gt_xdotdot[2][i1] =  b_a_RS_R_z[int(pt_01[1][i1])] #1403715274372140000\n",
    "\n",
    "        gt_o[0][i1] =  q_RS_w[int(pt_01[1][i1])] #1403715274372140000\n",
    "        gt_o[1][i1] =  q_RS_x[int(pt_01[1][i1])] #1403715274372140000\n",
    "        gt_o[2][i1] =  q_RS_y[int(pt_01[1][i1])] #1403715274372140000\n",
    "        gt_o[3][i1] =  q_RS_z[int(pt_01[1][i1])] #1403715274372140000\n",
    "\n",
    "        #gt_odot[0][i1] =  b_w_RS_S_x[int(pt_01[1][i1])] #1403715274372140000\n",
    "        #gt_odot[1][i1] =  b_w_RS_S_y[int(pt_01[1][i1])] #1403715274372140000\n",
    "        #gt_odot[2][i1] =  b_w_RS_S_z[int(pt_01[1][i1])] #1403715274372140000\n",
    "        \n",
    "    _im = torch.empty((count, image_dim, image_dim), dtype=torch.double)\n",
    "    for i1 in range(count):\n",
    "        _im[i1] = _images[int(pt_01[0][i1])] # I guess make sure that we only use images that are alligned with the appropriate index? (it's been a while since I wrote this lol)\n",
    "    del _images\n",
    "\n",
    "    gt_x_ = torch.from_numpy(gt_x).float()\n",
    "    gt_xdot_ = torch.from_numpy(gt_xdot).float()\n",
    "    \n",
    "    x,y,z = quat2eul(gt_o[0],gt_o[1],gt_o[2],gt_o[3])\n",
    "    x = torch.from_numpy(x).unsqueeze(0).float()\n",
    "    y = torch.from_numpy(y).unsqueeze(0).float()\n",
    "    z = torch.from_numpy(z).unsqueeze(0).float()\n",
    "\n",
    "    gt_o_ = torch.concat((x,y,z),dim=0).permute(1,0)\n",
    "\n",
    "\n",
    "            \n",
    "tt = torch.zeros(1330,3)\n",
    "ttdx = torch.zeros(1330,3)\n",
    "tto = torch.zeros(1330,3)\n",
    "ttdo = torch.zeros(1330,3)\n",
    "\n",
    "for i in range(1330):\n",
    "    x1 = gt_x_[:,i]\n",
    "    x2 = gt_x_[:,i+1]\n",
    "    x3 = gt_x_[:,i+2]\n",
    "    tt[i] = x2-x1\n",
    "    ttdx[i] = (x3 + x1 - 2 * x2)\n",
    "    \n",
    "    o1 = gt_o_[i,:]\n",
    "    o2 = gt_o_[i+1,:]\n",
    "    o3 = gt_o_[i+2,:]\n",
    "    do1 = torch.from_numpy(np.unwrap(o2-o1))\n",
    "    do2 = torch.from_numpy(np.unwrap(o3-o2))\n",
    "    tto[i] = do1\n",
    "    ttdo[i] = (torch.from_numpy(np.unwrap(do2-do1)))\n",
    "    \n",
    "max_dx = torch.max(tt,dim=0).values\n",
    "min_dx = torch.min(tt,dim=0).values\n",
    "\n",
    "max_ddx = torch.max(ttdx,dim=0).values\n",
    "min_ddx = torch.min(ttdx,dim=0).values\n",
    "\n",
    "max_do = torch.max(tto,dim=0).values\n",
    "min_do = torch.min(tto,dim=0).values\n",
    "\n",
    "max_ddo = torch.max(ttdo,dim=0).values\n",
    "min_ddo = torch.min(ttdo,dim=0).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latent_depth = 512\n",
    "num_windows = latent_depth//(4)\n",
    "\n",
    "l_history = torch.ones(3,4,16) #torch.ones(4,8,16)\n",
    "\n",
    "numVstates = 18\n",
    "v_history = torch.rand((4,1,numVstates)) #.repeat(1,8,1)\n",
    "numLfeatures = 128*2 # latent_depth//4 #256\n",
    "e = 4\n",
    "d = 256\n",
    "emb = 30\n",
    "\n",
    "ukf_t = ukf_.UKF(numLfeatures, numVstates, l_history, v_history, e, d, emb,3)\n",
    "ukf_o = ukf_.UKF(numLfeatures, numVstates, l_history, v_history, e, d, emb,3)\n",
    "\n",
    "image_t1 = 0\n",
    "image_t2 = 0\n",
    "latent = torch.randn((1, 1, numLfeatures)).unsqueeze(-1)\n",
    "latent_ = torch.randn((latent_depth,28,28)).unsqueeze(0)\n",
    "x_vehicle = torch.randn((numVstates,1)).unsqueeze(0).unsqueeze(0)\n",
    "dt = 0\n",
    "P_xx_prior_o = torch.eye(numVstates)*torch.abs(torch.rand(numVstates)).unsqueeze(0)\n",
    "P_xx_prior_t = torch.eye(numVstates)*torch.abs(torch.rand(numVstates)).unsqueeze(0)\n",
    "l_size = numLfeatures #256\n",
    "P_xxl_prior = torch.eye(l_size)*torch.abs(torch.rand(l_size)).unsqueeze(0)\n",
    "\n",
    "se = ukf_.SE_mapped(latent_depth,4)\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "vio = odom.vio_net(batch_size)\n",
    "\n",
    "kernel = [3, 3, 3, 3, 3, 3]\n",
    "stride = [1, 1, 1, 1, 1, 1]\n",
    "padding = [1, 1, 1, 1, 1, 1]\n",
    "        \n",
    "kernel_pool = [3, 3, 3, 3]\n",
    "stride_pool = [2, 2, 2, 2]\n",
    "padding_pool = [1, 1, 1, 1]\n",
    "\n",
    "kernel_inv = [2, 2, 2]\n",
    "stride_inv = [2, 2, 2]\n",
    "padding_inv = [0, 0, 0]\n",
    "dim = [1, 64, 128, 256, 512, 1024]\n",
    "\n",
    "depth = odom.x_net_decomp(dim, kernel, stride, padding, kernel_pool, stride_pool, padding_pool, kernel_inv, stride_inv, padding_inv)\n",
    "\n",
    "integrator = ukf_.dynamic_model_vehicle()\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "depth.to(device)\n",
    "vio.to(device)\n",
    "integrator = ukf_.dynamic_model_vehicle().to(device)\n",
    "PATH = 'depth_1_07_23'\n",
    "depth.load_state_dict(torch.load(PATH, map_location=device))\n",
    "\n",
    "\n",
    "optim_odom = optim.NAdam(vio.parameters(), lr = 0.001, betas = (0.9,0.99)) \n",
    "\n",
    "if 1:\n",
    "    with torch.no_grad():\n",
    "        latent_precomp = torch.zeros((1337,512,28,28), requires_grad=False)\n",
    "        with torch.no_grad():\n",
    "            drop = False\n",
    "            noise = 0\n",
    "            for i1 in range(len(_im)):\n",
    "                x = _im[i1].float().cuda().unsqueeze(0).unsqueeze(0)\n",
    "                o4, o3, o2, o1 = depth.encode( x, drop ) # deep to shallow latent\n",
    "                L, _o3, _o2 = depth.latent( o4, o3, o2, False, noise)\n",
    "\n",
    "                latent_precomp[i1,:,:,:] = L.cpu()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mina and max values for normalization\n",
    "\n",
    "\"\"\"\n",
    "r = 1330\n",
    "mark = torch.zeros(r,6)\n",
    "mark2 = torch.zeros(r,3)\n",
    "if 1:\n",
    "    for i1 in range(r):\n",
    "        x1 = gt_x_[:,i1]\n",
    "        x2 = gt_x_[:,i1+1]\n",
    "        x3 = gt_x_[:,i1+2]\n",
    "\n",
    "        dx1 = x2-x1\n",
    "        dx2 = x3-x2\n",
    "        dx1=(dx1-min_dx)/(max_dx-min_dx)\n",
    "        dx2=(dx2-min_dx)/(max_dx-min_dx)\n",
    "        mark[i1] = torch.concat((dx1, dx2),dim=0)\n",
    "\n",
    "        ddx = dx1-dx2\n",
    "        mark2[i1] = ddx #(ddx-min_accel)/(max_accel-min_accel) #(ddx-min_ddx)/(max_ddx-min_ddx)\n",
    "\n",
    "max_accel = torch.max(mark2,dim=0).values\n",
    "min_accel = torch.min(mark2,dim=0).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for loss calcuations and retrieving the vehicle state between time K:K+2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def d_loss(id, xdot, pdo, pddx, pddo, loss_metric):\n",
    "    x1 = gt_x_[:,id]\n",
    "    x2 = gt_x_[:,id+1]\n",
    "    x3 = gt_x_[:,id+2]\n",
    "\n",
    "    dx1 = x2-x1\n",
    "    dx2 = x3-x2\n",
    "    dx1=(dx1-min_dx)/(max_dx-min_dx)\n",
    "    dx2=(dx2-min_dx)/(max_dx-min_dx)\n",
    "    dx = torch.concat((dx1.unsqueeze(0), dx2.unsqueeze(0)),dim=0)\n",
    "\n",
    "    pred_dx = xdot[1] - xdot[0]\n",
    "    \n",
    "    ddx = dx2-dx1\n",
    "    ddx = (ddx-min_accel)/(max_accel-min_accel) #(ddx-min_ddx)/(max_ddx-min_ddx)\n",
    "    \n",
    "\n",
    "    if 0:\n",
    "        o1 = gt_o_[id,:]\n",
    "        o2 = gt_o_[id+1,:]\n",
    "        o3 = gt_o_[id+2,:]\n",
    "\n",
    "        do1 = o2 - o1\n",
    "        do2 = o3 - o2\n",
    "        do1=(do1-min_do)/(max_do-min_do)\n",
    "        do2=(do2-min_do)/(max_do-min_do)\n",
    "        do1 = torch.from_numpy(np.unwrap(do1.numpy()))\n",
    "        do2 = torch.from_numpy(np.unwrap(do2.numpy()))\n",
    "        do = torch.concat((do1, do2), dim=0)\n",
    "\n",
    "        ddo = do2 - do1\n",
    "        ddo = (ddo-min_ddo)/(max_ddo-min_ddo)\n",
    "    \n",
    "    loss = loss_metric(xdot, dx.cuda())*2 + loss_metric(pred_dx, ddx.cuda()) # + loss_metric(pddo, ddo.cuda())+ loss_metric(pdo, do.cuda()) \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def vehicle_state_embed(id,dt):\n",
    "    x1 = gt_x_[:,id]\n",
    "    x2 = gt_x_[:,id+1]\n",
    "    x3 = gt_x_[:,id+2]\n",
    "    \n",
    "    o1 = gt_o_[id,:]\n",
    "    o2 = gt_o_[id+1,:]\n",
    "    o3 = gt_o_[id+2,:]\n",
    "    \n",
    "    dx1 = (x2-x1)/dt\n",
    "    dx = (x3-x2)/dt\n",
    "    ddx = (dx-dx1)/dt\n",
    "    \n",
    "    do1 = (o2-o1)/dt\n",
    "    do = (o3-o2)/dt\n",
    "    ddo = (do-do1)/dt\n",
    "    \n",
    "    state = torch.concat((x3,dx,ddx,o3,do,ddo), dim=0).reshape(1,1,1,18)\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio = odom.vio_net(3)\n",
    "vio.to(device)\n",
    "optim_odom = optim.NAdam(vio.parameters(), lr = 0.0001, betas = (0.9,0.99))\n",
    "ukf_t.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average odom loss for epoch 1  for baseline and UKF: 0.11715626965463162 14.384377348795534\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "UKF training block\n",
    "\n",
    "\"\"\"\n",
    "ukf_l_history = torch.zeros((3,1,256))\n",
    "depth_history = torch.zeros((3,512,28,28))\n",
    "vehicle_history = torch.zeros((3,1,18))\n",
    "\n",
    "vio = odom.vio_net(3)\n",
    "vio.to(device)\n",
    "optim_odom = optim.NAdam(vio.parameters(), lr = 0.0001, betas = (0.9,0.99))\n",
    "\n",
    "PATH = 'vio_size_2' # 'encoder_state_dict_OGRE_stacked_im' 'encoder_state_dict_OGRE_stacked_im' \n",
    "vio.load_state_dict(torch.load(PATH, map_location=device))\n",
    "\n",
    "numVstates = 18\n",
    "numLfeatures = 256 # latent_depth//4 #256\n",
    "e = 4\n",
    "d = 256\n",
    "emb = 30\n",
    "h = 4\n",
    "dt = 0.05\n",
    "\n",
    "id_list = np.arange(500)\n",
    "l_sans_ukf = 0\n",
    "l_ukf_sum = 0\n",
    "l = 0\n",
    "\n",
    "ukf_t = ukf_.UKF(numLfeatures, numVstates, ukf_l_history, vehicle_history, e, d, emb, h)\n",
    "optim_ukf = optim.NAdam(ukf_t.parameters(), lr = 0.0001, betas = (0.9,0.99))\n",
    "ukf_t.to(device)\n",
    "\n",
    "loss_metric = nn.MSELoss()\n",
    "batch = 5\n",
    "\n",
    "for i1 in range(1):\n",
    "    np.random.shuffle(id_list)\n",
    "    for i2 in range(len(id_list)):\n",
    "        id = id_list[i2]\n",
    "        \"\"\"\n",
    "        collect dynamic estimates F(x) and measurement model H(x) approximations\n",
    "        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            depth_history = torch.zeros((3,512,28,28))\n",
    "            for i3 in range(3):\n",
    "                for i4 in range(3):\n",
    "                    depth_history[i4] = latent_precomp[id+i3+i4].detach().cpu()\n",
    "                _, p, _, _ = vio.encode(depth_history.reshape(-1,28,28).cuda())#o, t = vio.encode(latent_history.reshape(-1,28,28)) # latent space from time k-1 to k-4\n",
    "                ukf_l_history[i3] = p\n",
    "                vehicle_history[i3] = vehicle_state_embed(id+i3, dt)\n",
    "\n",
    "        ukf_t.latent_history = ukf_l_history.reshape(3,4,64).cuda()\n",
    "        ukf_t.vehicle_history = vehicle_history.cuda()\n",
    "        P_xx_prior_t = ukf_t.P_init_s.cuda()\n",
    "        P_xxl_prior_t = ukf_t.P_init_l.cuda()\n",
    "        p_inp = ukf_l_history[i3].unsqueeze(0).unsqueeze(-1).cuda()\n",
    "        x_vehicle = vehicle_history[i3].reshape(1,1,18,1)\n",
    "\n",
    "        t_hat, sigma_Y_t, P_xx_t, latent_hat_t, sigma_Y_l_t, P_xxl_t, l_hat_mean_t, t_hat_mean = ukf_t.predictor(p_inp, x_vehicle.cuda(), dt, P_xx_prior_t, P_xxl_prior_t)\n",
    "        # sigma_Y_t is the sigma points made from the input vector to the update equation. basically ignore t_hat\n",
    "\n",
    "        #need to pass sigma_Y_l_t through the vio model to get a \"measurement estimate\" odom value.\n",
    "        p = sigma_Y_l_t.permute(2,0,3,1)\n",
    "        x = torch.zeros((p.shape[0],2,3)).squeeze()\n",
    "        for i in range(p.shape[0]):\n",
    "            z = p[i].squeeze()\n",
    "            x[i],_,_,_ = vio.forward(_, z, _, z)\n",
    "        state_Y_hat = torch.mean(x,dim=0)\n",
    "\n",
    "        \"\"\"\n",
    "        Measurement update\n",
    "\n",
    "        \"\"\"\n",
    "        latent_k = latent_precomp[id+i3+i4+1].unsqueeze(0)\n",
    "        odom_measurment = torch.concat((latent_k, depth_history[:-1,:,:,:]),dim=0)\n",
    "        _, Y_gt, _, _ = vio.encode(odom_measurment.reshape(-1,28,28).cuda())\n",
    "\n",
    "        \"vvv train ukf dynamic model to make accurate predictions of future states \"\n",
    "        loss_latent_ukf = loss_metric(l_hat_mean_t.squeeze(), Y_gt.squeeze()) \n",
    "        \"vvv train dynamic model and redout layer to make interpreble latent spaces: Need to convert state_Y_hat from 2x3 to 1x18 vehicle state\"\n",
    "        #loss_state_ukf = loss_metric(state_Y_hat, t_hat_mean)\n",
    "\n",
    "        \"\"\"\n",
    "        UKF update\n",
    "\n",
    "        \"\"\"\n",
    "        latentY = Y_gt.squeeze().cuda() # \"measurement\"\n",
    "        Pxxl = P_xxl_t\n",
    "        sigmaYl = latent_hat_t # sigma points from F(Xl_sigma)\n",
    "        latentHat = sigma_Y_l_t # sigma points from H(X_sigma)\n",
    "        stateY = vehicle_state_embed(id+i3+i4+1, dt).cuda() # measured vehciel state\n",
    "        Pxx = P_xx_t\n",
    "        sigmaYp = t_hat\n",
    "        phat = sigma_Y_t\n",
    "        X_kpose_post, X_klatent_post, P_kp_post, P_kl_post, K_kp, K_kl = ukf_t.update(phat, sigmaYp, Pxx, stateY, latentHat, sigmaYl, Pxxl, latentY)\n",
    "        \"\"\"\n",
    "        Get error from updated state to train ukf\n",
    "\n",
    "        \"\"\"\n",
    "        xdot, _, _, _ = vio.forward(_, X_klatent_post, _, _)\n",
    "        temp, _, _, _ = vio.forward(_, Y_gt.clone().detach(), _, Y_gt.clone().detach())\n",
    "\n",
    "        l_ukf_update = d_loss(id+i3+i4+1, xdot, 0,0,0,loss_metric)\n",
    "        l_ukf_sum += l_ukf_update.clone().detach().cpu().numpy()\n",
    "        l_sans_ukf += d_loss(id+i3+i4+1, temp, 0, 0, 0, loss_metric).clone().detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "\n",
    "        l += l_ukf_update + loss_latent_ukf\n",
    "        \n",
    "        if i2%batch == 0 and i2>0:\n",
    "            optim_ukf.zero_grad()\n",
    "            (l/batch).backward()\n",
    "            optim_ukf.step()\n",
    "            del l_ukf_update\n",
    "            del loss_latent_ukf\n",
    "            del stateY\n",
    "            del l\n",
    "            l = 0\n",
    "    print('Average odom loss for epoch',i1+1,' for baseline and UKF:', l_sans_ukf/(i2+1), l_ukf_sum/(i2+1))\n",
    "    l_ukf_sum = 0\n",
    "    l_sans_ukf = 0\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5101, -0.0140,  0.2607],\n",
      "        [ 0.9193,  0.6306,  0.6268]], device='cuda:0')\n",
      "tensor([[0.4581, 0.0158, 0.2445],\n",
      "        [0.9866, 0.4825, 0.5883]], device='cuda:0',\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor(0.1988, device='cuda:0') 0.20897414\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f62a648e040>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Everything below this is just random stuff I wrote in the process of making this\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#print(xdot, temp)\n",
    "latent_im = X_klatent_post.reshape(16,16).detach().cpu().numpy()\n",
    "gt_latent_im = Y_gt.reshape(16,16).detach().cpu().numpy()\n",
    "with torch.no_grad():\n",
    "    xdot, _, _, _ = vio.forward(_, X_klatent_post, _, X_klatent_post)\n",
    "    print(xdot)\n",
    "    print(temp)\n",
    "    ukf_l = d_loss(id+i3+i4+1, xdot, 0,0,0,loss_metric).detach().cpu().numpy()\n",
    "    gt_l = d_loss(id+i3+i4+1, temp, 0, 0, 0, loss_metric).clone().detach().cpu().numpy()\n",
    "    print(ukf_l, gt_l)\n",
    "    \n",
    "\n",
    "#print(ukf_t.R_kp)\n",
    "print()\n",
    "#print(ukf_t.R_kl)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.imshow((latent_im-gt_latent_im))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.imshow(gt_latent_im)\n",
    "plt.colorbar(cmap=gt_latent_im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18]) torch.Size([256]) torch.Size([1, 18, 18]) torch.Size([1, 1, 256, 256]) torch.Size([1, 18, 18]) torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_kpose_post.shape, X_klatent_post.shape, P_kp_post.shape, P_kl_post.shape, K_kp.shape, K_kl.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physical torch.Size([1, 1, 36, 18]) torch.Size([1, 1, 36, 18])\n",
      "latent torch.Size([512, 1, 256, 1])\n",
      "torch.Size([512, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('physical',t_hat.shape, sigma_Y_t.shape)\n",
    "print('latent', sigma_Y_l_t.permute(2,0,3,1).shape)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0703, device='cuda:0')\n",
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(2.3263, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(p_inp.min())\n",
    "\n",
    "print(P_xxl_prior_t.min())\n",
    "\n",
    "print(P_xx_prior_t.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3197180577.py, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [14], line 52\u001b[0;36m\u001b[0m\n\u001b[0;31m    ukf_t.vehicle_history =\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "image = torch.zeros((1,1,448,448))\n",
    "latent_prior = torch.zeros((1,512,28,28))\n",
    "latent_history = torch.zeros((3,512,28,28))\n",
    "\n",
    "loss_metric = nn.MSELoss() # nn.L1Loss()\n",
    "dt = 0.05\n",
    "\n",
    "id_list = np.arange(600)\n",
    "id_list = np.append(id_list,(np.arange(200)+900))\n",
    "\n",
    "\n",
    "ukf_train = False\n",
    "odom_train = False\n",
    "batch_size = 10\n",
    "loss_odom_sum = 0\n",
    "\n",
    "l_avg = 0\n",
    "for i3 in range(5):\n",
    "    np.random.shuffle(id_list)\n",
    "    l_avg = 0\n",
    "    for i1 in range(1):#len(id_list)): # as of right now I don't need it to loop cause its just embedding the first 4 states and predictiing a single timestep \n",
    "        id = id_list[i1]\n",
    "        for i2 in range(3):\n",
    "            latent_history[i2] = latent_precomp[id+i2].detach().cpu() # ((latent_precomp[id+i2] - l_min)/(l_max-l_min)).detach().cpu()\n",
    "            \n",
    "\n",
    "        x_hat_ = torch.zeros(1,1,2,18).cuda()\n",
    "        #x = gt_x_[id+i2].cuda() #x_hat_[0,0,0:2,0:3].cuda()\n",
    "        #o = gt_o_[id+i2].cuda() #x_hat_[0,0,0:2,9:12].cuda()\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Perform everyhting up to measurement step\n",
    "            1. collect latent vector at time k-1\n",
    "            2. predict state at time k\n",
    "\n",
    "        \"\"\"\n",
    "        if ukf_train:\n",
    "            latent_Y_odom = latent_history\n",
    "            r, p, v, a = vio.encode(latent_Y_odom.reshape(-1,28,28).cuda())#o, t = vio.encode(latent_history.reshape(-1,28,28)) # latent space from time k-1 to k-4\n",
    "            p_inp = p.reshape(1,1,256,1).cuda() # just a bunch of unsqueezes really\n",
    "            #t = t.reshape(1,1,256,1)    \n",
    "            \n",
    "            x_vehicle = vehicle_state_embed(id+3, dt).cuda()\n",
    "            print(x_vehicle.device)\n",
    "            P_xx_prior_t = ukf_t.P_init_s\n",
    "            P_xxl_prior_t = ukf_t.P_init_l\n",
    "            \n",
    "            #P_xx_prior_o = ukf_o.P_init_s\n",
    "            #P_xxl_prior_o = ukf_o.P_init_l\n",
    "            ukf_t.latent_history = latent_history\n",
    "            ukf_t.vehicle_history = \n",
    "            t_hat, sigma_Y_t, P_xx_t, latent_hat_t, sigma_Y_l_t, P_xxl_t, l_hat_mean_t, t_hat_mean = ukf_t.predictor(p_inp, x_vehicle, dt, P_xx_prior_t, P_xxl_prior_t)\n",
    "            #o_hat, sigma_Y_o, P_xx_o, latent_hat_o, sigma_Y_l_o, P_xxl_o, l_hat_mean_o, o_hat_mean = ukf_o.predictor(o, x_vehicle, dt, P_xx_prior_o, P_xxl_prior_o)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Collect measurements at time k\n",
    "            1. get new latent space from depth network for frame k\n",
    "            2. get new odom latenet space for time k\n",
    "            3. get odometry \"measurement\" (prediction) for time k\n",
    "            4. integrate outputs with dynamic model to predict current x, p, xdot, pdot\n",
    "\n",
    "        \"\"\"\n",
    "        #latent_k = latent_precomp[id_list[i1] + 4].unsqueeze(0)\n",
    "        #latent_Y_odom = torch.concat((latent_k, latent_history[:-1,:,:,:]),dim=0) # latent space from time k to k-3\n",
    "        latent_Y_odom = latent_history #latent_precomp[id_list[i1] + i2].detach().cpu()\n",
    "        #print(latent_Y_odom.shape)\n",
    "        r, p, v, a = vio.encode(latent_Y_odom.reshape(-1,28,28).cuda())\n",
    "        dx = r\n",
    "        do = p\n",
    "        ddx = v\n",
    "        ddo = a\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print('r, p, v, a', r.shape, p.shape, v.shape, a.shape)\n",
    "        xdot,xdotdot,pdot,pdotdot = vio.forward(r, p, v, a)\n",
    "        #print('dx,ddx, do,ddo', xdot.shape, xdotdot.shape, pdot.shape, pdotdot.shape)\n",
    "        \n",
    "        loss_odom = d_loss(id, xdot, 0, 0, 0, loss_metric) #pdot.flatten(), xdotdot.flatten(), pdotdot.flatten(), loss_metric)\n",
    "        \n",
    "        \n",
    "        if ukf_train:\n",
    "            latent_Y_t = p # Set the \"measurement\" as the actual latent space for translation and orientation branches of the odometry network\n",
    "            #latent_Y_o = r\n",
    "        \n",
    "        if odom_train:\n",
    "            #loss_odom, state_Y = integration_loss(x_hat_, xdot,xdotdot,pdot,pdotdot, id + 4, dt, loss_metric)\n",
    "            loss_dx = 0\n",
    "            #print(loss_odom)\n",
    "            loss_odom_sum += loss_odom\n",
    "            l_avg += loss_odom.clone().detach().cpu().numpy()\n",
    "\n",
    "            if i1%batch_size == 0 and i1 > 0:\n",
    "                optim_odom.zero_grad()\n",
    "                (loss_odom_sum/batch_size).backward()\n",
    "                optim_odom.step()\n",
    "                del r\n",
    "                del p\n",
    "                del xdot\n",
    "                del xdotdot\n",
    "                del pdot\n",
    "                del pdotdot\n",
    "                del loss_odom\n",
    "                del loss_odom_sum\n",
    "                loss_odom_sum = 0\n",
    "                #del state_Y\n",
    "                del x\n",
    "                del o\n",
    "                del x_hat_\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Perform ukf update step\n",
    "        1. update new odom latent space based on dynamic prediction vs truth\n",
    "        2. predict new odom based on updated rp\n",
    "        3. make new odom predicitons and backprop loss\n",
    "\n",
    "        \"\"\" \n",
    "        if ukf_train:\n",
    "            XT_post, XT_latent_post, P_kp_post, P_kl_post, K_kp, K_kl = ukf_t.update(t_hat, sigma_Y_t, P_xx_t, state_Y, latent_hat_t, sigma_Y_l_t, P_xxl_t, latent_Y_t)\n",
    "            #XO_post, XO_latent_post, P_kp_post, P_kl_post, K_kp, K_kl = ukf_o.update(o_hat, sigma_Y_o, P_xx_o, state_Y, latent_hat_o, sigma_Y_l_o, P_xxl_o, latent_Y_o)\n",
    "            xdot ,_, _, _ = vio.forward(0, XT_latent_post, 0, 0)\n",
    "            \n",
    "            #optim_ukf.zero_grad()\n",
    "            #loss_ukf,_ = integration_loss(x_hat_, xdot,xdotdot,pdot,pdotdot, id, dt, loss_metric)\n",
    "            #loss_ukf.backward()\n",
    "            #optim_ukf.step()\n",
    "\n",
    "        \n",
    "    #print('Average odom loss epoch',i3+1,':', l_avg / len(id_list))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Testing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "image = torch.zeros((1,1,448,448))\n",
    "latent_prior = torch.zeros((1,512,28,28))\n",
    "latent_history = torch.zeros((3,512,28,28))\n",
    "\n",
    "loss_metric = nn.MSELoss()\n",
    "dt = 0.05\n",
    "\n",
    "id_list = np.arange(400)\n",
    "np.random.shuffle(id_list)\n",
    "ukf_train = False\n",
    "batch_size = 3\n",
    "loss_odom_sum = 0\n",
    "\n",
    "x_list = np.zeros((1330,3))\n",
    "x_dot_list = np.zeros((1330,3))\n",
    "x_hat_ = torch.zeros(1,1,2,18).cuda()\n",
    "l_avg = 0\n",
    "with torch.no_grad():\n",
    "    for i1 in range(1330): # as of right now I don't need it to loop cause its just embedding the first 4 states and predictiing a single timestep \n",
    "        for i2 in range(3):\n",
    "            latent_history[i2] = latent_precomp[i1 + i2].detach().cpu()\n",
    "\n",
    "\n",
    "        id = 1 # np.random.randn(1300)\n",
    "        x_hat_ = torch.zeros(1,1,2,18).cuda()\n",
    "        x = x_hat_[0,0,0:2,0:3].cuda()\n",
    "        o = x_hat_[0,0,0:2,9:12].cuda()\n",
    "\n",
    "        \"\"\"\n",
    "        Perform everyhting up to measurement step\n",
    "            1. collect latent vector at time k-1\n",
    "            2. predict state at time k\n",
    "\n",
    "        \"\"\"\n",
    "        if ukf_train:\n",
    "            o, t = vio.encode(latent_history.reshape(-1,28,28)) # latent space from time k-1 to k-4\n",
    "            o = o.reshape(1,1,256,1) # just a bunch of unsqueezes really\n",
    "            t = t.reshape(1,1,256,1)    \n",
    "            #  \n",
    "            P_xx_prior_t = ukf_t.P_init_s\n",
    "            P_xxl_prior_t = ukf_t.P_init_l\n",
    "            P_xx_prior_o = ukf_o.P_init_s\n",
    "            P_xxl_prior_o = ukf_o.P_init_l\n",
    "\n",
    "            t_hat, sigma_Y_t, P_xx_t, latent_hat_t, sigma_Y_l_t, P_xxl_t, l_hat_mean_t, t_hat_mean = ukf_t.predictor(o, x_vehicle, dt, P_xx_prior_t, P_xxl_prior_t)\n",
    "            o_hat, sigma_Y_o, P_xx_o, latent_hat_o, sigma_Y_l_o, P_xxl_o, l_hat_mean_o, o_hat_mean = ukf_o.predictor(t, x_vehicle, dt, P_xx_prior_o, P_xxl_prior_o)\n",
    " \n",
    "\n",
    "        \"\"\"\n",
    "        Collect measurements at time k\n",
    "            1. get new latent space from depth network for frame k\n",
    "            2. get new odom latenet space for time k\n",
    "            3. get odometry \"measurement\" (prediction) for time k\n",
    "            4. integrate outputs with dynamic model to predict current x, p, xdot, pdot\n",
    "\n",
    "        \"\"\"\n",
    "        latent_k = latent_precomp[i1 + 4].unsqueeze(0)\n",
    "        latent_Y_odom = latent_history #torch.concat((latent_k, latent_history[:-1,:,:,:]),dim=0) # latent space from time k to k-3\n",
    "        \n",
    "        r, p, v, a = vio.encode(latent_Y_odom.reshape(-1,28,28).cuda())\n",
    "        dx = r\n",
    "        do = p\n",
    "        ddx = v\n",
    "        ddo = a\n",
    "        #print('r, p, v, a', r.shape, p.shape, v.shape, a.shape)\n",
    "        xdot,xdotdot,pdot,pdotdot = vio.forward(r, p, v, a)\n",
    "        \n",
    "        x_list[i1] = xdot[0].detach().cpu().numpy()\n",
    "        x_dot_list[i1] = xdot[1].detach().cpu().numpy() - xdot[0].detach().cpu().numpy()\n",
    "        \n",
    "        if ukf_train:\n",
    "            latent_Y_t = p # Set the \"measurement\" as the actual latent space for translation and orientation branches of the odometry network\n",
    "            latent_Y_o = r\n",
    "        \n",
    "        #loss_odom, state_Y = integration_loss(x_hat_, xdot,xdotdot,pdot,pdotdot, i1, dt, loss_metric)\n",
    "        #l_avg += loss_odom.clone().detach().cpu().numpy()\n",
    "\n",
    "        del r\n",
    "        del p\n",
    "        del xdot\n",
    "        del xdotdot\n",
    "        del pdot\n",
    "        del pdotdot\n",
    "       # del loss_odom\n",
    "        #del state_Y\n",
    "        del x\n",
    "        del o\n",
    "        #del x_hat_\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Perform ukf update step\n",
    "        1. update new odom latent space based on dynamic prediction vs truth\n",
    "        2. predict new odom based on updated rp\n",
    "        3. make new odom predicitons and backprop loss\n",
    "\n",
    "        \"\"\"\n",
    "        if ukf_train:\n",
    "            XT_post, XT_latent_post, P_kp_post, P_kl_post, K_kp, K_kl = ukf_t.update(t_hat, sigma_Y_t, P_xx_t, state_Y, latent_hat_t, sigma_Y_l_t, P_xxl_t, latent_Y_t)\n",
    "            XO_post, XO_latent_post, P_kp_post, P_kl_post, K_kp, K_kl = ukf_o.update(o_hat, sigma_Y_o, P_xx_o, state_Y, latent_hat_o, sigma_Y_l_o, P_xxl_o, latent_Y_o)\n",
    "            xdot,xdotdot,pdot,pdotdot = vio.forward(XO_latent_post, XT_latent_post)\n",
    "\n",
    "            #optim_ukf.zero_grad()\n",
    "            loss_ukf,_ = integration_loss(x_hat_, xdot,xdotdot,pdot,pdotdot, id, dt, loss_metric)\n",
    "            #loss_ukf.backward()\n",
    "            #optim_ukf.step()\n",
    "\n",
    "print('done')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "axis = np.arange(1330)\n",
    "plt.plot(axis, x_dot_list)\n",
    "print(vio.biases_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = 0\n",
    "axis = np.arange(1330)\n",
    "axis2 = np.arange(1337)\n",
    "print(x_list.shape)\n",
    "tt = torch.zeros(1330,3)\n",
    "ttdx = torch.zeros(1330,3)\n",
    "\n",
    "\n",
    "for i in range(1330):\n",
    "    x1 = gt_x_[:,i]\n",
    "    x2 = gt_x_[:,i+1]\n",
    "    x3 = gt_x_[:,i+2]\n",
    "    tt[i] = x2-x1\n",
    "    dx1=(x1-min_dx)/(max_dx-min_dx)\n",
    "    dx2=(x2-min_dx)/(max_dx-min_dx)\n",
    "    dx3=(x3-min_dx)/(max_dx-min_dx)\n",
    "    dddd = ((dx3-dx2) - (dx2-dx1))\n",
    "    ttdx[i] = (dddd-min_accel)/(max_accel-min_accel)\n",
    "    \n",
    "v=1\n",
    "plt.figure(1)\n",
    "plt.plot(axis,((tt[:,v]-min_dx[v])/(max_dx[v]-min_dx[v])).numpy(), axis, x_list[:,v]+0.15)\n",
    "plt.figure(2)\n",
    "plt.plot(axis, ttdx[:,v], axis, x_dot_list[:,v]+0.15)\n",
    "print(ttdx.max(), x_dot_list.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336.861696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#PATH = 'vio_size_2' \n",
    "#torch.save(vio.state_dict(), PATH)\n",
    "PATH = 'ukf_dx' \n",
    "torch.save(ukf_t.state_dict(), PATH)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated()/1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Average odom loss?', l_avg / 1330)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Precompute the latent spaces since theyre static \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "if 1:\n",
    "    latent_precomp = torch.zeros((1337,512,28,28), requires_grad=False)\n",
    "    with torch.no_grad():\n",
    "        drop = False\n",
    "        noise = 0\n",
    "        for i1 in range(len(_im)):\n",
    "            x = _im[i1].float().cuda().unsqueeze(0).unsqueeze(0)\n",
    "            o4, o3, o2, o1 = depth.encode( x, drop ) # deep to shallow latent\n",
    "            L, _o3, _o2 = depth.latent( o4, o3, o2, False, noise)\n",
    "\n",
    "            latent_precomp[i1,:,:,:] = L\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('dt', (int(pose_timestamp[int(pt_01[1][1])]) - int(pose_timestamp[int(pt_01[1][0])]))/np.power(10,9))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "kernel = [3, 3, 3, 3, 3, 3]\n",
    "stride = [1, 1, 1, 1, 1, 1]\n",
    "padding = [1, 1, 1, 1, 1, 1]\n",
    "        \n",
    "kernel_pool = [3, 3, 3, 3]\n",
    "stride_pool = [2, 2, 2, 2]\n",
    "padding_pool = [1, 1, 1, 1]\n",
    "\n",
    "kernel_inv = [2, 2, 2]\n",
    "stride_inv = [2, 2, 2]\n",
    "padding_inv = [0, 0, 0]\n",
    "dim = [1, 64, 128, 256, 512, 1024]\n",
    "\n",
    "depth = odom.x_net_decomp(dim, kernel, stride, padding, kernel_pool, stride_pool, padding_pool, kernel_inv, stride_inv, padding_inv)\n",
    "\n",
    "integrator = ukf_.dynamic_model_vehicle()\n",
    "batch_size = 4\n",
    "vio = odom.vio_net(batch_size)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "PATH = 'depth_1_07_23' # 'encoder_state_dict_OGRE_stacked_im' 'encoder_state_dict_OGRE_stacked_im' \n",
    "depth.load_state_dict(torch.load(PATH, map_location=device))\n",
    "        \n",
    "depth.to(device)    \n",
    "vio = vio.to(device)\n",
    "integrator.to(device)\n",
    "\n",
    "l_history = torch.ones(4,4,16) #torch.ones(4,8,16)\n",
    "numVstates = 18\n",
    "v_history = torch.rand((4,1,numVstates)) #.repeat(1,8,1)\n",
    "numLfeatures = 128 # latent_depth//4 #256\n",
    "\n",
    "ukf = ukf_.UKF(numLfeatures, numVstates, l_history, v_history)\n",
    "    \n",
    "#from itertools import chain\n",
    "    \n",
    "#optimizer = optim.NAdam(chain(vio.parameters(), xnet.parameters()), lr = 0.0001, betas = (0.9,0.99))\n",
    "optimizer = optim.NAdam(vio.parameters(), lr = 0.0001, betas = (0.9,0.99))\n",
    "optimizer_depth = optim.NAdam(xnet.parameters(), lr = 0.001, betas = (0.9,0.99))\n",
    "\n",
    "dt = (int(pose_timestamp[int(pt_01[1][1])]) - int(pose_timestamp[int(pt_01[1][0])]))/np.power(10,9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Primary odometry training block\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 3 # Note: really the length of image the sequence, so 1 < batch_size < inf\n",
    "index = np.arange(450)\n",
    "t = torch.zeros(6)\n",
    "pc_xyz_pre = torch.zeros(batch_size,3,56,56)\n",
    "loss = 0\n",
    "scale_ms = 1\n",
    "scale_3d = 0.2\n",
    "l_avg = 0\n",
    "camfer_ish = False\n",
    "alt_loss = False\n",
    "\n",
    "print('Training for',epochs,'epochs with a range of',len(index),'samples using',device)\n",
    "\n",
    "for i1 in range(epochs):\n",
    "    random.shuffle(index)\n",
    "    count = 0\n",
    "    for i2 in range(len(index)//int(batch_size)):\n",
    "            \n",
    "            for i3 in range(int(batch_size)): # collect data in batches of batch_size\n",
    "                idx = index[i2 * int(batch_size) + i3]\n",
    "                inp = _im[idx].unsqueeze(0).float().cuda()\n",
    "\n",
    "                for i4 in range(6):\n",
    "                        #t[i4] = _gt[i4][idx+1] - _gt[i4][idx]\n",
    "                        t[i4] = t_norm[idx][i4] # Wait, t_norm is the precomputed rot+translations...\n",
    "                #inp = gray.get_gray(inp.cuda()).squeeze().unsqueeze(0) / 256.0\n",
    "\n",
    "                if i3 == 0:\n",
    "                    op_batch = t.unsqueeze(0)\n",
    "                    inp_batch = inp.unsqueeze(0)\n",
    "                else:\n",
    "                    if i3 < batch_size - 1:\n",
    "                        op_batch = torch.concat((op_batch, t.unsqueeze(0))) # list of odometry translations and rotations\n",
    "                    inp_batch = torch.concat((inp_batch, inp.unsqueeze(0))) # array of input images\n",
    "                    \n",
    "                    \n",
    "            o4, o3, o2, _ = x_net.encode(inp_batch,False)\n",
    "            latent, _o3, _o2 = x_net.latent( o4, o3, o2, False,0)\n",
    "            latent = latent.reshape(int(latent.shape[1]*batch_size), latent.shape[2], latent.shape[2])\n",
    "            pose_pred, rot_pred = vio.forward(latent) # for now just do one pair of images\n",
    "\n",
    "            #rot_pred = rot_pred*0\n",
    "            #odom_pred = torch.concat((pose_pred.reshape(batch_size-1,-1).unsqueeze(0), rot_pred.reshape(batch_size-1,-1).unsqueeze(0)), dim=2)\n",
    "            odom_pred = torch.concat((pose_pred.unsqueeze(0), rot_pred.unsqueeze(0)), dim=1)\n",
    "           \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            l_avg += loss.clone().detach().cpu().numpy()\n",
    "            count += 1\n",
    "            optimizer.step()\n",
    "\n",
    "            del inp\n",
    "            del op_batch\n",
    "            del inp_batch\n",
    "            del o2\n",
    "            del o3\n",
    "            del o4\n",
    "            del _o2\n",
    "            del _o3\n",
    "            #del depth_pred\n",
    "            del latent\n",
    "            del odom_pred\n",
    "            #del loss_odom\n",
    "            del loss\n",
    "            loss = 0\n",
    "            del t\n",
    "            t = torch.zeros(6)\n",
    "    if i1%2 == 0:\n",
    "        print('Avg loss for epoch',i1+1,' is:',l_avg/count)\n",
    "    l_avg = 0\n",
    "    count = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if 0:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Loop: requires 4 latenet spaces for embedding, and the next 4 latenet spaces for prediciton.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for i1 in range(1):\n",
    "    \n",
    "    if update:\n",
    "        X_kpose_post, X_klatent_post, P_kp_post, P_kl_post, K_kp, K_kl\n",
    "        latent_K_prior = latent_Y\n",
    "        x_vehicle = X_kpose_post # Maybe use gt_Y?\n",
    "        P_xx_prior = P_xx_post\n",
    "        P_xxl_prior = P_xxl_post\n",
    "    \n",
    "    # squeeze latent space to ukf size (128). This is from image at time K-1\n",
    "    l_squeezed = se.encode(latent_K_prior)\n",
    "    # Perform everything up to measurement step\n",
    "    p_hat, sigma_Y_p, P_xx, latent_hat, sigma_Y_l, P_xxl, l_hat_mean, p_hat_mean = ukf.predictor(l_squeezed, x_vehicle, dt, P_xx_prior, P_xxl_prior)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Collect measurements at time K. This is basically the traditional training routine\n",
    "    latent_Y = depth.latent(depth.encode(image_K), 0) # squeeze latent space to ukf size (128). This is from image at time K\n",
    "    xdot,xdotdot,pdot,pdotdot = vio(latent_Y_x4) # Collect odometry \"measurement\" for time K. This takes in a x4 latenet space from time [K:k-3]\n",
    "    \n",
    "    x = x_hat_[0,0,0,0:3]\n",
    "    p = x_hat_[0,0,0,9:12]\n",
    "    \n",
    "    pred = torch.concat((x,xdot,xdotdot,p,pdot,pdotdot), dim=0).reshape(1,1,1,18)\n",
    "    Y_hat_ = integrator(pred, 0.1)\n",
    "    \n",
    "    optimizer_odom.zero_grad()\n",
    "    loss_odom = loss_metric(state_Y, gt_Y)\n",
    "    loss_odom.backward()\n",
    "    optimizer_odom.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Perform ukf update step\n",
    "    latent_Y_squeezed = se.encode(latent_Y) # Encode actual latent space for update step\n",
    "    X_kpose_post, X_klatent_post, P_xx_post, P_xxl_post, K_kp, K_kl = ukf.update(p_hat, sigma_Y_p, P_xx, state_Y, latent_hat, sigma_Y_l, P_xxl, latent_Y)\n",
    "    latenet_post = se.decode(X_klatent_post, latent_) # perform excite based on ukf update remapping to original latenet space (512)\n",
    "    latent_Y_x4_training = torch.concat((latent_Y_x4[1:3,:,:],latent_post), dim=0) # for training substitute with posteriori state\n",
    "    state_Y_training = vio(latent_Y_x4_training)\n",
    "    \n",
    "    \n",
    "    x = x_hat_[0,0,0:2,0:3]\n",
    "    p = x_hat_[0,0,0:2,9:12]\n",
    "    pred = torch.concat((x,xdot,xdotdot,p,pdot,pdotdot), dim=1).reshape(1,2,1,18)\n",
    "    x_hat_1 = integrator(pred[0,0,0,:].reshape(1,1,1,18), dt)\n",
    "    x_2 = pred[0,1,0,:]\n",
    "    m = (x_2 + x_hat_1)/2\n",
    "    pred[0,1,0,3:9] = m[0,0,0,3:9]\n",
    "    pred[0,1,0,12:18] = m[0,0,0,12:18]\n",
    "    x_hat_2 = integrator(pred[0,1,0,:].reshape(1,1,1,18), dt)\n",
    "    x_hat_ = torch.concat((x_hat_1, x_hat_2), dim=2)\n",
    "    x_hat_x = x_hat_2[0,0,0,0:6]\n",
    "    x_hat_o = x_hat_2[0,0,0,9:12]\n",
    "    gt_Y_x = torch.concat((gt_x_[:,id]-gt_x_[:,id-1], gt_xdot_[:,id]-gt_xdot_[:,id-1]))\n",
    "    gt_Y_o = gt_o_[:,id] - gt_o_[:,id-1]\n",
    "    \n",
    "    # Back prop\n",
    "    optimizer_ukf.zero_grad()\n",
    "    loss_attn = loss_metric(x_hat_x, gt_Y_x) + loss_metric_o(x_hat_o, gt_Y_o)\n",
    "    loss_attn.backward()\n",
    "    optimizer_ukf.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foo",
   "language": "python",
   "name": "my_kernel_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c23caf59b7a145a3d8a2045a295f4c2fdde335662298f46d08c971156aa4749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
